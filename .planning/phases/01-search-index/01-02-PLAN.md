---
phase: 01-search-index
plan: 02
type: execute
wave: 2
depends_on: ["01-01"]
files_modified:
  - crates/relay/src/server.rs
  - crates/relay/Cargo.toml
autonomous: false

must_haves:
  truths:
    - "The relay server starts up, indexes all existing documents into the search index, and serves search queries"
    - "GET /search?q=<query> returns JSON with ranked results including doc_id, title, folder, snippet, and score"
    - "GET /search returns 503 while initial indexing is in progress"
    - "Editing a document causes the search index to reflect the change within a few seconds (debounced)"
    - "Documents removed from folder metadata are removed from the search index"
    - "An empty query returns an empty results array with total_hits 0"
  artifacts:
    - path: "crates/relay/src/server.rs"
      provides: "SearchIndex field on Server, startup indexing, live update hook, HTTP search endpoint"
      contains: "search_index"
    - path: "crates/relay/Cargo.toml"
      provides: "tempfile dependency for index directory (if needed)"
  key_links:
    - from: "Server::startup_reindex"
      to: "SearchIndex::add_document"
      via: "iterate folder docs for metadata, iterate content docs for body"
      pattern: "search_index.*add_document"
    - from: "webhook callback"
      to: "SearchIndex background worker"
      via: "mpsc channel notification on document update"
      pattern: "search_index.*on_document_update"
    - from: "handle_search (axum handler)"
      to: "SearchIndex::search"
      via: "spawn_blocking for sync tantivy operations"
      pattern: "spawn_blocking.*search"
    - from: "Server routes()"
      to: "handle_search"
      via: "axum .route(\"/search\", get(handle_search))"
      pattern: "route.*search.*get"
---

<objective>
Wire the SearchIndex into the relay server: startup indexing of all documents, live update notifications via the existing webhook callback, a background debounced worker for incremental reindexing, and the HTTP search endpoint.

Purpose: Makes search actually usable — the core module from Plan 01 is pure logic; this plan connects it to real Y.Doc content and exposes it via HTTP.
Output: A working `GET /search?q=<query>&limit=20` endpoint on the relay server.
</objective>

<execution_context>
@/home/penguin/.claude/get-shit-done/workflows/execute-plan.md
@/home/penguin/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/01-search-index/01-CONTEXT.md
@.planning/phases/01-search-index/01-RESEARCH.md
@.planning/phases/01-search-index/01-01-SUMMARY.md
@crates/relay/src/server.rs (Server struct, routes(), startup_reindex(), webhook callback)
@crates/y-sweet-core/src/link_indexer.rs (parallel pattern: on_document_update, run_worker, debounce, startup reindex)
@crates/y-sweet-core/src/search_index.rs (SearchIndex from Plan 01)
</context>

<tasks>

<task type="auto">
  <name>Task 1: Add SearchIndex to Server with startup indexing and live updates</name>
  <files>crates/relay/src/server.rs, crates/relay/Cargo.toml</files>
  <action>
    Integrate SearchIndex into the Server struct following the exact same pattern as LinkIndexer. This is a parallel subsystem, not a replacement.

    **1. Server struct changes:**
    - Add `search_index: Option<Arc<SearchIndex>>` field to `Server` struct (line ~181, alongside `link_indexer`)
    - Add `search_ready: Arc<std::sync::atomic::AtomicBool>` field for 503-during-indexing gate

    **2. Server::new() changes (line ~184):**
    - Create a temporary directory for the tantivy index (use `std::env::temp_dir().join("lens-relay-search-index")`)
    - Clean the directory on startup (remove and recreate) to ensure fresh index
    - Construct `SearchIndex::new(&index_path)?`
    - Wrap in `Arc<SearchIndex>` and store in Server
    - Initialize `search_ready` as `AtomicBool::new(false)`
    - Set up the debounced background worker for search index updates:
      - Create an mpsc channel (capacity 1000, same as link indexer)
      - Spawn a tokio task running the search index worker
      - The worker follows the EXACT same debounce pattern as LinkIndexer::run_worker:
        - Receives doc_id from channel
        - Checks if it's a folder doc or content doc
        - Content docs: debounce 2 seconds then reindex
        - Folder docs: process immediately, detect title changes, reindex affected content docs
      - For content doc reindex: read Y.Text("contents") body, look up title from folder metadata, call search_index.add_document()
      - For folder doc updates: detect added/removed docs (diff filemeta), add/remove from search index accordingly
    - Store the sender half of the channel for use in the webhook callback

    **3. startup_reindex() changes (line ~502):**
    After the existing `link_indexer.reindex_all_backlinks()` call, add search index initial build:
    - Find all folder docs (reuse `find_all_folder_docs` pattern from link_indexer or make it pub)
    - Build a uuid-to-metadata map: iterate filemeta_v0 in each folder doc to get (uuid -> title, folder_name)
      - Title extraction: strip leading "/" and trailing ".md", take basename (same pattern as link_indexer detect_renames lines 468-477)
      - Folder name: derive from the folder doc's own identity or hardcode based on known folder doc IDs. For now, use "Lens" for the first folder doc and "Lens Edu" for the second. A better approach: check if the folder doc key contains a known relay ID pattern, or simply label based on iteration order of `find_all_folder_docs()` results.
    - For each entry in the uuid-to-metadata map:
      - Build the full doc_id from relay_id + uuid
      - Look up the content doc in the DashMap
      - Read Y.Text("contents") body
      - Call `search_index.add_document(uuid, title, body, folder_name)`
    - After all documents indexed, set `search_ready.store(true, Ordering::Release)`
    - Log count of documents indexed

    **4. Webhook callback changes (line ~330):**
    In the document update callback (the closure that already notifies link_indexer), add a parallel notification for the search index:
    - Clone the search index sender
    - After the existing `link_indexer.on_document_update()` call pattern, send the doc_id to the search index channel
    - Guard with `should_index()` check (same as link indexer) to avoid infinite loops

    **5. Make `find_all_folder_docs` and `is_folder_doc` accessible:**
    These functions in link_indexer.rs are currently private (no `pub`). Either:
    - Make them `pub` in link_indexer.rs (preferred — they are general-purpose utilities)
    - Or duplicate the logic in the search worker

    Make `find_all_folder_docs`, `is_folder_doc`, and `extract_id_from_filemeta_entry` public in link_indexer.rs.

    **6. Background worker implementation:**
    The search index worker needs its own struct or can be a standalone async function. Create a `SearchWorker` in search_index.rs (or a separate file) with:
    - `pending: Arc<DashMap<String, Instant>>` for debounce tracking
    - `tx: mpsc::Sender<String>` for enqueuing updates
    - `on_document_update(&self, doc_id: &str)` method (same debounce-send pattern as LinkIndexer)
    - `run_worker(rx, docs, search_index)` async method

    The worker logic for content doc reindex:
    ```
    1. Read Y.Text("contents") from content doc -> body
    2. Find which folder doc contains this UUID (scan all folder docs' filemeta_v0)
    3. Extract title from folder doc filemeta path
    4. Call search_index.add_document(uuid, title, body, folder) inside spawn_blocking
    ```

    The worker logic for folder doc updates:
    ```
    1. Scan filemeta_v0 to get current set of (uuid, path) pairs
    2. Compare with previous snapshot (maintain a cache like LinkIndexer.filemeta_cache)
    3. For removed UUIDs: call search_index.remove_document(uuid)
    4. For added UUIDs: queue them for content indexing
    5. For renamed UUIDs: reindex with new title
    ```

    **Cargo.toml:** If tempfile is needed for index path, add it. Check if it's already a dependency. (It's used in server.rs for file uploads, so likely already available via the relay crate.)
  </action>
  <verify>
    ```bash
    # Verify it compiles
    CARGO_TARGET_DIR=~/code/lens-relay/.cargo-target cargo check --manifest-path=crates/Cargo.toml
    ```
  </verify>
  <done>Server compiles with SearchIndex integrated. The startup_reindex function builds the search index from all loaded documents. The webhook callback notifies the search index worker on document updates.</done>
</task>

<task type="auto">
  <name>Task 2: Add HTTP search endpoint</name>
  <files>crates/relay/src/server.rs</files>
  <action>
    Add the `GET /search` HTTP endpoint to the relay server.

    **1. Search query params struct:**
    ```rust
    #[derive(Deserialize)]
    struct SearchQuery {
        q: String,
        #[serde(default = "default_search_limit")]
        limit: usize,
    }
    fn default_search_limit() -> usize { 20 }
    ```

    **2. Handler function:**
    ```rust
    async fn handle_search(
        State(server_state): State<Arc<Server>>,
        Query(params): Query<SearchQuery>,
    ) -> Result<Json<Value>, AppError> {
        // Check if search is ready (503 during initial indexing)
        if !server_state.search_ready.load(std::sync::atomic::Ordering::Acquire) {
            return Err(AppError(
                StatusCode::SERVICE_UNAVAILABLE,
                anyhow!("Search index is being built, please try again shortly"),
            ));
        }

        let limit = params.limit.min(100); // Cap at 100
        let q = params.q.trim().to_string();

        if q.is_empty() {
            return Ok(Json(json!({
                "results": [],
                "total_hits": 0,
                "query": ""
            })));
        }

        let search_index = server_state.search_index.clone()
            .ok_or_else(|| AppError(
                StatusCode::SERVICE_UNAVAILABLE,
                anyhow!("Search index not available"),
            ))?;

        // Run search in blocking context (tantivy is sync)
        let results = tokio::task::spawn_blocking(move || {
            search_index.search(&q, limit)
        })
        .await
        .map_err(|e| AppError(StatusCode::INTERNAL_SERVER_ERROR, e.into()))?
        .map_err(|e| AppError(StatusCode::INTERNAL_SERVER_ERROR, e))?;

        let total_hits = results.len();
        Ok(Json(json!({
            "results": results,
            "total_hits": total_hits,
            "query": params.q
        })))
    }
    ```

    **3. Route registration:**
    In `Server::routes()` (line ~694), add:
    ```rust
    .route("/search", get(handle_search))
    ```
    Place it alongside the other routes, before the `with_state` call.

    **4. Response format (from CONTEXT.md decisions):**
    Each result in the `results` array:
    ```json
    {
      "doc_id": "uuid-string",
      "title": "Document Name",
      "folder": "Lens",
      "snippet": "...context with <mark>matched</mark> terms...",
      "score": 3.14
    }
    ```

    Ensure the `SearchResult` struct from Plan 01 has `#[derive(Serialize)]` so it can be serialized to JSON directly. If not, map the results to serde_json::Value in the handler.
  </action>
  <verify>
    ```bash
    # Verify it compiles
    CARGO_TARGET_DIR=~/code/lens-relay/.cargo-target cargo check --manifest-path=crates/Cargo.toml

    # Run all tests
    CARGO_TARGET_DIR=~/code/lens-relay/.cargo-target cargo test --manifest-path=crates/Cargo.toml -- --nocapture
    ```
  </verify>
  <done>GET /search endpoint is registered in the router. Empty queries return empty results. Non-empty queries are dispatched to SearchIndex via spawn_blocking. Results include doc_id, title, folder, snippet, and score. 503 is returned during initial indexing.</done>
</task>

<task type="checkpoint:human-verify" gate="blocking">
  <what-built>
    Full search index integration: SearchIndex created at startup, populated from all Y.Docs, live-updated on document changes, and queryable via GET /search endpoint.
  </what-built>
  <how-to-verify>
    1. Start the relay server locally:
       ```bash
       pkill -f "relay serve" || true
       CARGO_TARGET_DIR=~/code/lens-relay/.cargo-target cargo run --manifest-path=crates/Cargo.toml --bin relay -- serve --port 8090
       ```
    2. Run the setup script to populate test documents:
       ```bash
       cd lens-editor && npm run relay:setup
       ```
    3. Test the search endpoint:
       ```bash
       # Basic search
       curl 'http://localhost:8090/search?q=test'

       # Should return JSON with results array, total_hits, query
       # Each result should have doc_id, title, folder, snippet, score

       # Empty query
       curl 'http://localhost:8090/search?q='
       # Should return {"results":[],"total_hits":0,"query":""}

       # Phrase search
       curl 'http://localhost:8090/search?q=%22exact+phrase%22'

       # Limit parameter
       curl 'http://localhost:8090/search?q=test&limit=1'
       # Should return at most 1 result
       ```
    4. Verify snippets contain `<mark>` tags (not `<b>` tags) around matched terms
    5. Check server logs for startup indexing count (should show how many documents were indexed)
  </how-to-verify>
  <resume-signal>Type "approved" if search works correctly, or describe any issues</resume-signal>
</task>

</tasks>

<verification>
```bash
# 1. All tests pass
CARGO_TARGET_DIR=~/code/lens-relay/.cargo-target cargo test --manifest-path=crates/Cargo.toml -- --nocapture

# 2. Server compiles and starts
CARGO_TARGET_DIR=~/code/lens-relay/.cargo-target cargo build --manifest-path=crates/Cargo.toml --bin relay

# 3. Search endpoint responds
curl -s 'http://localhost:8090/search?q=test' | python3 -m json.tool
```
</verification>

<success_criteria>
The relay server starts, indexes all documents from storage, and serves search queries via GET /search. Results are BM25-ranked with <mark>-tagged snippets. Live document edits cause the search index to update within a few seconds.
</success_criteria>

<output>
After completion, create `.planning/phases/01-search-index/01-02-SUMMARY.md`
</output>
